defaults:
  - lora_kitti360_strategy_a.yaml  # Inherit from Strategy A
  - _self_

# Strategy B Safe: LoRA on Depth Head + Late Transformer Layers (Reduced Memory)
# Conservative settings to prevent GPU driver crashes
exp_name: lora_kitti360_strategy_b_safe_r16

# LoRA Configuration - same targets as strategy_b
lora:
  enabled: True
  rank: 16
  alpha: 32
  dropout: 0.1
  # Target Depth Head + Late 8 layers of transformer (16-23 out of 24)
  target_modules:
    - "depth_head.*"                        # All depth head modules
    - "aggregator.frame_blocks.1[6-9].*"   # Frame blocks 16-19
    - "aggregator.frame_blocks.2[0-3].*"   # Frame blocks 20-23
    - "aggregator.global_blocks.1[6-9].*"  # Global blocks 16-19
    - "aggregator.global_blocks.2[0-3].*"  # Global blocks 20-23

# REDUCED batch size to avoid OOM
max_img_per_gpu: 16  # 2 batches of 8-image sequences (2 * 8 = 16)
accum_steps: 2       # Split into 2 gradient accumulation steps

optim:
  optimizer:
    lr: 5e-5                 # Lower LR for transformer fine-tuning
    weight_decay: 0.01

  frozen_module_names:
    - "*aggregator.frame_blocks.[0-9].*"    # Freeze blocks 0-9
    - "*aggregator.frame_blocks.1[0-5].*"   # Freeze blocks 10-15
    - "*aggregator.global_blocks.[0-9].*"   # Freeze blocks 0-9
    - "*aggregator.global_blocks.1[0-5].*"  # Freeze blocks 10-15
    - "*aggregator.patch_embed*"            # Freeze patch embedding
    - "*camera_head*"                       # Freeze camera head

  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["depth_head"]
        max_norm: 1.0
        norm_type: 2
      - module_name: ["aggregator.frame_blocks", "aggregator.global_blocks"]
        max_norm: 0.5  # Stricter clipping for transformer
        norm_type: 2

  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 5e-7
              end_value: 5e-5
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 5e-5
              end_value: 5e-7
          lengths: [0.15, 0.85]  # 15% warmup, 85% cosine decay
          interval_scaling: ['rescaled', 'rescaled']

max_epochs: 150  # Longer training for transformer adaptation

checkpoint:
  save_freq: 3  # Save less frequently
