defaults:
  - lora_kitti360_strategy_a.yaml  # Inherit from Strategy A
  - _self_

# Strategy B Ultra-Safe: Maximum stability for debugging GPU crashes
# This config prioritizes stability over training speed
exp_name: lora_kitti360_strategy_b_ultra_safe_r8

# LoRA Configuration - REDUCED rank for lower memory
lora:
  enabled: True
  rank: 8            # REDUCED from 16 to 8 (half the parameters)
  alpha: 16          # Keep alpha/rank = 2.0 ratio
  dropout: 0.1
  # Target only last 4 transformer layers (not 8)
  target_modules:
    - "depth_head.*"                        # All depth head modules
    - "aggregator.frame_blocks.2[0-3].*"   # Frame blocks 20-23 only
    - "aggregator.global_blocks.2[0-3].*"  # Global blocks 20-23 only

# AGGRESSIVE memory reduction
# max_img_per_gpu MUST be >= sequence length to avoid batch_size=0
max_img_per_gpu: 4        # Set equal to sequence length (minimum valid)
accum_steps: 1            # MUST be 1 when batch_size=1 (cannot chunk further!)

# Reduce sequence length
data:
  train:
    common_config:
      img_nums: [4, 4]    # REDUCED from [8, 8] to [4, 4] (50% reduction)
  val:
    common_config:
      img_nums: [4, 4]

optim:
  optimizer:
    lr: 2.5e-5           # Reduced LR due to smaller effective batch
    weight_decay: 0.01

  frozen_module_names:
    - "*aggregator.frame_blocks.[0-9].*"    # Freeze blocks 0-9
    - "*aggregator.frame_blocks.1[0-9].*"   # Freeze blocks 10-19
    - "*aggregator.global_blocks.[0-9].*"   # Freeze blocks 0-9
    - "*aggregator.global_blocks.1[0-9].*"  # Freeze blocks 10-19
    - "*aggregator.patch_embed*"            # Freeze patch embedding
    - "*camera_head*"                       # Freeze camera head

  # More conservative AMP settings
  amp:
    enabled: True
    amp_dtype: float16     # Use float16 instead of bfloat16 (better error detection)

  gradient_clip:
    _target_: train_utils.gradient_clip.GradientClipper
    configs:
      - module_name: ["depth_head"]
        max_norm: 0.5      # STRICTER clipping
        norm_type: 2
      - module_name: ["aggregator.frame_blocks", "aggregator.global_blocks"]
        max_norm: 0.3      # VERY strict clipping for transformer
        norm_type: 2

  options:
    lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
            - _target_: fvcore.common.param_scheduler.LinearParamScheduler
              start_value: 2.5e-7
              end_value: 2.5e-5
            - _target_: fvcore.common.param_scheduler.CosineParamScheduler
              start_value: 2.5e-5
              end_value: 2.5e-7
          lengths: [0.15, 0.85]
          interval_scaling: ['rescaled', 'rescaled']

# Conservative CUDA settings
cuda:
  cudnn_deterministic: True    # Disable for deterministic behavior
  cudnn_benchmark: False       # Disable benchmark for stability
  allow_tf32: False            # Disable TF32 for numerical precision

# More frequent validation to catch issues early
val_epoch_freq: 1

# Shorter epoch for faster failure detection
limit_train_batches: 100      # VERY SHORT for testing

max_epochs: 150

checkpoint:
  save_freq: 1  # Save every epoch for debugging
